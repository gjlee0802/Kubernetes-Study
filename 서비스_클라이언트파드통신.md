# 서비스: 클라이언트가 파드를 검색하고 통신을 가능하게 함
## 서비스
- 쿠버네티스의 서비스: 동일한 서비스를 제공하는 파드 그룹에 지속적인 단일 접점을 만들려고 할 때 생성하는 리소스.
- 각 서비스는 서비스가 존재하는 동안 절대 바뀌지 않는 IP주소, 포트가 있다.   
- 클라이언트는 해당 IP, 포트로 접속한 다음 해당 서비스를 지원하는 파드 중 하나로 연결된다.   
이런 방식으로 서비스의 클라이언트는 서비스를 제공하는 개별 파드의 위치를 알 필요가 없다.   

### 서비스 생성
**정확히 어떤 파드가 서비스의 일부분인지 아닌지를 정의하는 방법은?**   
레이블 셀렉터를 이용하면 된다.   
rc와 기타 파드 컨트롤러에서 레이블 셀렉터를 사용해 동일한 세트에 속하는 파드를 지정했듯이,   
서비스에도 동일한 메커니즘으로 소속 파드를 지정한다.   

생성하는 방법은 다음과 같다.   
- kubectl expose 명령으로 생성
  ~~~
  $ kubectl expose <pod_selector> <name> --type=<service_type> --name <service_name>
  example: 
  $ kubectl expose rc kubia --type=LoadBalancer --name kubia-http
  ~~~
  위에서 예시 명령은 "kubia"라는 이름의 RC을 노출시켜서 LoadBalancer 유형의 "kubia-http"라는 이름의 서비스를 생성한 것이다.
- YAML descriptor로 생성
  ~~~
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia
  spec:
    ports:
    - port: 80          # 서비스가 사용할 포트(노출될 포트)
      targetPort: 8080  # 서비스가 포워드할 컨테이너 포트
    selector:
      app: kubia        # app=kubia 레이블이 있는 모든 파드가 이 서비스에 속한다.
  ~~~
### 서비스 목록 확인하기
~~~
$ kubectl get svc
~~~
출력 결과에서 보이는 CLUSTER-IP 열의 IP 주소는 서비스에 할당된 클러스터 IP이므로    
클러스터 내부에서만 액세스할 수 있다.   
### 서비스로 요청 확인(테스트)하는 방법 
- 로그 검사: 서비스의 클러스터 IP로 요청을 보내고 응답을 로그로 남기는 파드를 만들기.
- ssh 접속: 쿠버네티스 노드로 ssh 접속하고 curl 명령을 실행하기.
- kubectl exec: k exec 명령어로 기존 파드에서 curl 명령을 실행하기.
#### 실행 중인 컨테이너에 원격으로 명령어 실행
kubectl exec 명령어를 사용하면 기존 파드의 컨테이너 내에서 원격으로 임의의 명령어 실행이 가능하다.   
~~~
$ kubectl exec <pod_name> -- <command>
example:
$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153
~~~
예시 명령어는 command를 실행시킬 파드를 정하고 서비스의 클러스터 IP로 HTTP 요청을 전송한 것이다.   
#### 명령어 실행 후 동작 과정
![스크린샷, 2021-05-15 05-57-37](https://user-images.githubusercontent.com/49184890/118349118-bcf12780-b589-11eb-9ed2-543ef4b200d3.png)   
서비스의 세션 어피니티를 지정하지 않았으므로 동일한 명령을 더 실행하면 동일한 클라이언트에서 요청하더라도    
서비스 프록시가 임의의 파드를 선택해 연결을 전달하기 때문에 요청할 때마다 다른 파드가 선택된다.   
#### 서비스의 세션 어피니티(sessionAffinity) 구성
세션 어피니티 속성값으로 다음 2가지를 지정할 수 있다.
- None          (default값이다.)
- ClientIP
서비스의 세션 어피니티 속성을 ClientIP로 설정하면 서비스 프록시는 동일한 클라이언트 IP의 모든 요청을 동일한 파드로 전달한다.   
~~~
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
  ...
~~~
위와 같이 spec 아래에 세션 어피니티를 지정한다.   
#### 서비스 정의: 멀티 포트 지정
~~~
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
~~~
#### 파드 정의: 포트 이름 사용
파드 리소스의 YAML 파일에서 파드의 포트에 이름을 붙여주자.   
~~~
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http          # Port 8080의 이름은 http이다.
      containerPort: 8080
    - name: https         # Port 8443의 이름은 https이다.
      containerPort: 8443
~~~
#### 서비스 정의: 포트 이름으로 포트 참조하기
이름으로 포트를 참조할 수 있다.   
~~~
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: http    # 앞서 파드 정의에서 8080에 http라는 이름을 부여했다.
  - name: https
    port: 443
    targetPort: https   # 앞서 파드 정의에서 8443에 https라는 이름을 부여했다.
~~~
### 서비스 검색 방법 2가지
쿠버네티스는 클라이언트 파드가 서비스의 IP와 포트를 검색할 수 있는 방법을 제공한다.   
방법으로 다음 2가지가 있다.   
- 환경변수를 통한 검색
- DNS를 통한 검색
#### _1. 서비스 검색 방법 1: 환경변수를 통한 서비스 검색_
파드가 시작되면 쿠버네티스는 해당 시점에 존재하는 각 서비스 정보를 환경변수로 초기화해준다. 한번 확인해보자.

> 환경변수 이름으로 표시될 때 서비스 이름의 대시는 밑줄로 변환되고 모든 문자가 대문자로 표시된다.

파드가 이미 실행되고 있는 상태에서 서비스를 만들었다면 해당 서비스에 대한 정보가 초기화되어있지 않기 때문에 파드를 지우고 다시 생성해야 한다. 



아래 명령어를 입력하여 대상 파드의 컨테이너 내부에서 env 명령어를 실행하여 환경변수를 조회한다.   
> 주의할 것은 파드를 만든 후에 서비스를 만들면 서비스에 대한 환경변수를 설정할 수 없는 것이다.    
> 파드가 이미 실행되고 있는 상태에서 서비스를 만들었다면 해당 서비스에 대한 정보가 초기화되어있지 않기 때문에 파드를 지우고 다시 생성해야 한다.   
따라서 kubectl delete pods --all 명령으로 rc나 rs가 파드를 새로 만들 수 있도록 한다.   
~~~
$ kubectl exec <pod_name> env | grep SERVICE
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=10.96.0.1
HOSTNAME_SERVICE_PORT=80
HOSTNAME_SERVICE_HOST=10.108.167.231
~~~
출력되는 환경변수 목록 중   
<service_name>_SERVICE_HOST는 서비스의 클러스터 IP다.   
<service_name>_SERVICE_PORT는 서비스가 제공되는 포트다.   

#### _2. 서비스 검색 방법 2: DNS를 통한 서비스 검색_
kube-system 네임스페이스에 있는 파드를 출력해보면 `coredns` 또는 `kube-dns`가 있다. 이 서비스는 클러스터에서 실행중인 모든 파드가 자동으로 사용하도록 구성되는 DNS 서버이다. 

> 쿠버네티스는 각 컨테이너의 `/etc/resolv.conf` 파일을 수정해서 쿠버네티스 내부에서 실행하는 DNS 서버를 설정한다. 

클라이언트 파드는 환경변수에 의존하는 대신 FQDN(정규화된 도메인 이름 형식)을 통해 서비스에 접근할 수 있다.

<br>

쿠버네티스 클러스터 내부에서 사용하는 로컬 서비스의 FQDN 형식은 다음과 같다.

```
{service-name}.{namespace}.svc.cluster.local

# hostname 서비스의 경우
hostname.default.svc.cluster.local
```

FQDN으로 kubia 서비스에 액세스하자.   
먼저 파드의 컨테이너 내에서 bash 셸을 실행하고 curl 명령어를 사용해 kubia 서비스에 액세스한다.   
~~~
$ kubectl exec -it kubia-3inly bash

root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local

root@kubia-3inly:/# curl http://kubia.default

root@kubia-3inly:/# curl http://kubia
~~~
파드 컨테이너 내부의 DNS resolver가 구성돼 있기 때문에 네임스페이스와 svc.cluster.local 접미사를 생략할 수 있다.   
컨테이너에서 /etc/resolv.conf 파일을 보면 이해할 수 있다.   
~~~
root@kubia-3inly:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local ...
~~~
   
### 클러스터 외부에 있는 서비스 연결   
지금까지는 클러스터 내부에서 실행 중인 하나 이상의 파드와의 통신을 지원하는 서비스였다.   
그러나 쿠버네티스 서비스 기능으로 외부 서비스를 노출하려는 경우가 있을 수 있다. 서비스가 클러스터 내에 있는 파드로 연결을 전달하는 게 아니라, 외부 IP와 포트로 연결을 전달하는 것이다.   
클러스터에서 실행 중인 클라이언트 파드는 내부 서비스에 연결하는 것처럼 외부 서비스에 연결할 수 있다.   

#### 서비스 엔드포인트 소개
서비스 엔드포인트 전에 먼저 서비스를 좀 더 살펴보자.   
서비스는 파드에 직접 연결되지 않는다. 대신 엔드포인트 리소스가 그 사이에 있다.   
kubectl describe 명령을 사용하여 엔드포인트를 확인할 수 있다.   
~~~
$ kubectl describe svc kubia
~~~
앤드포인트 리소스는 서비스로 노출되는 파드의 IP 주소와 포트 목록이다.   
~~~
$ kubectl get endpoints kubia
~~~
#### 서비스 엔드포인트 수동 구성
서비스의 엔드포인트를 서비스와 분리하면 엔드포인트를 수동으로 구성하고 업데이트할 수 있다.   
파드 셀렉터 없이 서비스를 만들면 쿠버네티스는 엔드포인트 리소스를 생성하지 않는다.   
다음과 같이 셀렉터 없이 서비스를 생성하고 셀렉터가 없는 서비스에 관한 엔드포인트 리소스를 생성해보자.   
- 셀렉터 없이 서비스 생성   
  ~~~
  apiVersion: v1
  kind: Service
  metadata:
    name: external-service  # 서비스의 이름은 엔드포인트 오브젝트 이름과 동일해야 한다.
  spec:                     # 셀렉터가 정의돼 있지 않다.
    ports:
    - port: 80
  ~~~
- 셀렉터가 없는 서비스에 관한 엔드포인트 리소스 생성   
  ~~~
  apiVersion: v1
  kind: Endpoints           # Endpoints는 별도의 리소스(서비스 속성 아님)
  metadata:
    name: external-service  # 엔드포인트 오브젝트의 이름은 서비스 이름과 일치해야 한다.
  subsets:
    - addresses:
      - ip: 11.11.11.11     # 서비스가 연결을 전달할 엔드포인트의 IP
      - ip: 22.22.22.22     # 서비스가 연결을 전달할 엔드포인트의 ip
      ports:
      - port: 80            # 엔드포인트의 대상 포트
  ~~~
엔드포인트 오브젝트는 서비스와 이름이 같아야 하고 서비스를 제공하는 대상 외부의 IP 주소와 포트 목록을 가져야 한다.   
서비스가 만들어진 후 만들어진 컨테이너에는 서비스의 환경변수가 포함되며 IP:Port 쌍에 대한 모든 연결은 서비스 엔드포인트 간에 로드밸런싱한다.   
![스크린샷, 2021-06-16 05-56-02](https://user-images.githubusercontent.com/49184890/122122369-ad564e80-ce67-11eb-8d89-c99812f4a877.png)   
#### 외부 서비스를 위한 별칭 생성
서비스의 엔드포인트를 수동으로 구성해 외부 서비스를 노출하는 대신 좀 더 간단한 방법으로 FQDN(정규화된 도메인 이름)으로 외부 서비스를 참조할 수 있다.   

- ExternelName 서비스 생성   
  외부 서비스의 별칭으로 사용되는 서비스를 만들려면 spec 내부의 type 필드를 ExternalName으로 설정해 서비스 리소스를 만든다.   
  api.somecompany.com에 공개 API가 있다고 가정하자. 다음 예제에 표시도니 대로 이를 가리키는 서비스를 정의할 수 있다.   
  ~~~
  apiVersion: v1
  kind: Service
  metadata:
    name: external-service
  spec:
    type: ExternalName
    externalName: api.somecompany.com
    ports:
    - port: 80
  ~~~
서비스가 생성되면 파드는 external-service.default.svc.cluster.local 도메인 이름으로 외부 서비스에 연결할 수 있다.   
이렇게 하면 서비스를 사용하는 파드에서 실제 서비스 이름과 위치가 숨겨져 나중에 externalName 속성을 변경하거나 유형을 다시 ClusterIP로 변경하고 서비스 스펙을 만들어 수정하면 다른 서비스를 가리키도록 할 수 있다.   
ExternalName 서비스는 DNS 레벨에서만 구현된다.   

### 외부 클라이언트에 서비스 노출
지금까지는 클러스터 내부에서 파드가 서비스를 사용하는 방법을 알아봤다. 그러나 다음 그림과 같이 Front-End 웹 서버와 같은 특정 서비스를 외부에 노출해 외부 클라이언트가 액세스할 수 있게 하고 싶을 수 있다.   
![스크린샷, 2021-06-16 06-30-53](https://user-images.githubusercontent.com/49184890/122126272-7898c600-ce6c-11eb-9b5d-a19d745f9411.png)   
#### 외부에서 서비스를 액세스할 수 있는 몇 가지 방법.   
- 1. **노드포트**로 서비스 유형 설정: NodePort 서비스의 경우 각 클러스터 노드는 노드 자체에서 포트를 열고 해당 포트로 수신된 트래픽을 서비스로 전달한다.   
- 2. 서비스 유형을 노드포트 유형의 확장인 **로드밸런서로 설정**: 로드밸런서로 서비스에 액세스할 수 있다.   
- 3. 단일 IP 주소로 여러 서비스를 노출하는 **인그레스 리소스** 만들기: HTTP 레벨(7L)에서 작동하므로 4L 서비스보다 더 많은 기능을 제공할 수 있다.   

#### 1. 노드포트 서비스 이용
서비스를 생성하고 유형을 노드포트로 설정한다. 노드포트 서비스를 만들면 쿠버네티스는 모든 노드에 특정 포트를 할당하고 서비스를 구성하는 파드로 들어오는 연결을 전달한다.   
일반 서비스(ClusterIP)와 유사하지만 서비스의 내부 Cluster IP 뿐만 아니라 모든 노드의 IP와 할당된 노드포트로 서비스에 액세스할 수 있다.   
이것은 노드포트 서비스와 상호작용할 때 더 큰 의미가 있다.   
   
- 노드포트 서비스 생성   
  ~~~
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-nodeport
  spec:
    type: NodePort        # 서비스 유형을 노드포트로 설정
    ports:
    - port: 80            # 서비스 내부 클러스터 IP의 포트
      targetPort: 8080    # 서비스 대상 파드의 포트
      nodePort: 30123     # 각 클러스터 노드의 포트 30123으로 서비스에 액세스할 수 있다.
    selector:
      app: kubia
  ~~~
  유형을 포트로 설정, 이 서비스가 모든 클러스터 노드에 바인딩돼야 하는 노드포트를 지정한다.   
  노드포트를 반드시 지정해야 하는 것은 아니다. 생략하면 쿠버네티스가 임의의 포트를 선택할 것이다.   
   
- 노드포트 서비스 확인
  ~~~
  $ kubectl get svc kubia-nodeport
  NAME            CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE
  kubia-nodeport  10.111.254.223  <nodes>        80:30123/TCP   2m
  ~~~
  EXTERNAL-IP 열을 보면 <nodes>라고 표시돼 있고 클러스터 노드의 IP 주소로 서비스에 액세스할 수 있음을 나타낸다. PORT(S) 열에는 클러스터 IP의 내부 포트(80)과 노드포트(30123) 모두 표시된다.   
  이 서비스는 다음 주소에서 액세스할 수 있다.   
  - 10.111.254.223:80   
  - <첫 번째 노드의 IP>:30123   
  - <두 번째 노드의 IP>:30123 등   
<br>
다음 그림은 두 노드의 포트 30123에 노출된 서비스를 보여준다.    
이런 포트에 대한 수신 연결은 임의로 선택된 파드로 전달되며, 연결 중인 노드에서 실행 중인 포트일 수도 있고 아닐 수도 있다.   
  
![스크린샷, 2021-06-16 07-39-18](https://user-images.githubusercontent.com/49184890/122132544-ff9e6c00-ce75-11eb-86dd-4bb6787eca2b.png)   
~~~
 $ curl http://130.211.97.55:30123
~~~  
> 그러나 문제점이 있다. 노드에 장애가 있을 경우 하나의 노드에만 요청을 보낸다면 오프라인 상태인 노드에 요청을 보내게 된다. 따라서 로드밸런서를 앞에 두는 것이 현명하다.   
#### 2. 외부 로드밸런서로 서비스 노출
클라우드 공급자(AWS,GCP,Azure...)에서 실행되는 쿠버네티스 클러스터는 일반적으로 클라우드 인프라에서 로드밸런서를 자동으로 **프로비저닝(노드 장애를 대비하여 모든 노드에 요청을 분산시키고 오프라인 상태인 노드로 요청을 보내지 않도록)하는 기능**을 제공한다.   
노드포트 대신 서비스 유형을 로드밸런서로 설정하기만 하면 된다.   
로드밸런서는 공개적으로 액세스 가능한 고유한 IP 주소를 가지며 모든 연결을 서비스로 전달한다. 로드밸런서의 IP 주소로 서비스에 액세스할 수 있다.   
> 로드밸런스 서비스는 노드포트 서비스의 확장이다.   
- 로드밸런서 서비스 생성
  다음과 같은 YAML로 로드밸런서 유형 서비스를 생성한다.
  ~~~
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-loadbalancer
  spec:
    type: LoadBalancer
    ports:
    - port: 80
      targetPort: 8080
    selector:
      app: kubia
  ~~~
- 로드밸런서를 통한 서비스 연결   
  서비스를 생성한 후 클라우드 인프라가 로드밸런서를 생성하고 IP 주소를 서비스 오브젝트에 쓴다.   
  작업이 완료되면 로드밸런서 IP 주소가 서비스의 external IP 주소로 표시된다.   
  ~~~
  $ kubectl get svc kubia-loadbalancer
  $ curl <EXTERNAL-IP>
  ~~~
다음 그림과 같이 외부 클라이언트는 로드밸런서의 Port 80에 연결하고 노드에 암묵적으로 할당된 노드포트로 라우팅된다.   
![스크린샷, 2021-06-16 08-08-33](https://user-images.githubusercontent.com/49184890/122134692-1b0b7600-ce7a-11eb-9312-0912d481fdd1.png)   
그림을 위의 노드포트 서비스와 비교해보면 **로드밸런서 유형 서비스는 추가 인프라 제공 로드밸런서가 있는 노드포트 서비스**라는 것을 알 수 있다.    
`kubectl describe svc kubia-loadbalancer` 명령으로 서비스에 대한 추가 정보를 보면 서비스에 노드포트가 선택됐음을 알 수 있다.   

#### 추가설명: 외부 연결의 특성 이해
- 1. 불필요한 네트워크 홉의 이해와 예방.   
    파드에 도달하려면 추가적인 네트워크 홉이 필요할 수 있으며 이것이 항상 바람직한 것은 아니다.   
    외부의 연결을 수신한 노드에서 실행 중인 파드로만 외부 트래픽을 전달하도록 서비스를 구성해 이 추가 홉을 방지할 수 있다.
    ~~~
    spec:
        externTrafficPolicy: Local
        …
    ~~~
    서비스 정의에 이 설정이 포함돼 있고 서비스의 노드포트로 외부 연결이 열린 경우 서비스 프록시는 로컬에 실행 중인 파드를 선택한다.   
    이 어노테이션을 사용하면 단점이 있다. 일반적으로 연결은 모든 파드에 균등하게 분산되지만 이 어노테이션을 사용할 때는 더 이상 적용되지 않는다(로컬 외부 트래픽 정책 서비스의 문제점).   
    ![스크린샷, 2021-06-16 22-31-17](https://user-images.githubusercontent.com/49184890/122228282-a7597f80-cef2-11eb-8b98-3b4d59ae1aac.png)
   
    
- 2. 클라이언트 IP가 보존되지 않음 인식.  
    일반적으로 클러스터 내의 클라이언트가 서비스로 연결할 때 서비스의 파드는 클라이언트의 IP 주소를 얻을 수 있다. 그러나 노드포트로 연결을 수신하면 패킷에서 소스 네트워크 주소 변환(SNAT)이 수행되므로 패킷의 Source IP가 변경된다.   
    파드는 실제 클라이언트 IP를 볼 수 없다. 이는 클라이언트의 IP를 알아야 하는 일부 애플리케이션에서 문제가 될 수 있다. (ex. 웹 서버의 경우 액세스 로그에 브라우저의 IP를 표시하지 못함을 의미).  
   이전 절에서 설명한 로컬 외부 트래픽 정책은 연결을 수신하는 노드와 대상 파드를 호스팅하는 노드 사이에 추가 홉이 없기 때문에 클라이언트 IP 보존이 가능하다(SNAT이 수행되지 않음).   
#### 3. 인그레스 리소스로 서비스 외부 노출
로드밸런서 서비스는 자신의 공용 IP 주소를 가진 로드밸런서가 필요하지만, 인그레스는 한 IP 주소로 수십 개의 서비스에 접근이 가능하도록 지원해준다. 클라이언트가 HTTP 요청을 인그레스에 보낼 때, 요청한 호스트와 경로에 따라 요청을 전달할 서비스가 결정된다.   
![스크린샷, 2021-06-16 22-32-17](https://user-images.githubusercontent.com/49184890/122228399-c526e480-cef2-11eb-9177-02241a83393b.png)
   
   
- 인그레스 컨트롤러 실행.   
    인그레스 리소스를 작동시키려면 클러스터에 인그레스 컨트롤러를 실행해야 한다. 쿠버네티스 환경마다 다른 컨트롤러 구현을 사용할 수 있지만 일부는 기본 컨트롤러를 전혀 제공하지 않는다.   

- 인그레스 리소스 생성.   
    클러스터에서 인그레스 컨트롤러가 실행 중인 것을 확인하고 인그레스 리소스를 만든다.   
    ~~~
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: kubia
    spec:
      rules:
      - host: kubia.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: kubia-nodeport
              servicePort: 80
    ~~~
    Host kubia.example.com으로 요청되는 인그레스 컨트롤러에 수신된 모든 HTTP 요청을 Port 80의 kubia-nodeport 서비스로 전송하도록 하는 인그레스 규칙을 정의했다.   
- 인그레스로 서비스 액세스.   
    http://kubia.example.com 서비스에 액세스하려면 도메인 이름이 인그레스 컨트롤러의 IP와 매핑되도록 화인해야 한다.   
    인그레스의 IP 주소를 찾기 위해 인그레스 목록을 다음 명령으로 확인한다.   
    ~~~
    $ kubectl get ingresses
    ~~~
    인그레스의 IP 주소를 찾았으면 kubia.example.com을 해당 IP로 확인하도록 DNS 서버를 구성하거나, 다음 줄을 /etc/hosts에 추가할 수 있다.   
    ~~~
    <ingress-IP>    <kubia.example.com>
    ~~~
    
    이제 모든 것이 설정됐으므로 브라우저나 curl을 사용하여 http://kubia.example.com 서비스에 액세스할 수 있다.   
    ~~~
    $ curl http://kubia.example.com
    ~~~
    
- 인그레스 동작 방식.   
    ![스크린샷, 2021-06-16 22-33-04](https://user-images.githubusercontent.com/49184890/122228510-e091ef80-cef2-11eb-9e55-77617e4b3a3f.png)
   
- 하나의 인그레스로 여러 서비스 노출   
    인그레스 스펙을 자세히 보면 규칙과 경로가 모두 배열이므로 여러 항목을 가질 수 있다.   
    인그레스는 여러 호스트와 경로를 여러 서비스에 매핑할 수 있다.   
       
    **동일한 호스트의 다른 경로로 여러 서비스 매핑**.   
    다음 예제에 표시된 것처럼 동일한 호스트의 여러 경로를 다른 서비스에 매핑할 수 있다.   
    ~~~
    …
        - host: kubia.example.com
          http:
            paths: 
            - path: /kubia
              backend:
                serviceName: kubia
                servicePort: 80
            - path: /bar
              backend:
                serviceName: bar
                servicePort: 80
    ~~~
    **서로 다른 호스트로 서로 다른 서비스 매핑하기**   
    다음 예제와 같이 경로 대신 HTTP 요청의 호스트를 기반으로 서로 다른 서비스를 매핑할 수 있다.   
    ~~~
    spec:
        rules:
        - host: foo.example.com
          http:
            paths:
            - path: /
              backend:
                serviceName: foo
                servicePort: 80
        - host: bar.example.com
          http:
            paths:
            - path: /
              backend:
                serviceName: bar
                servicePort: 80
    ~~~
- TLS(Transport Layer Security) 트래픽을 처리하도록 인그레스 구성   
    앞서 인그레스가 HTTP 트래픽을 전달하는 방법을 봤다.   
    HTTPS의 경우에는 TLS를 지원하도록 인그레스를 구성하는 방법을 살펴보아야 한다.   
    **인그레스를 위한 TLS 인증서 생성**   
    클라이언트가 인그레스 컨트롤러에 대한 TLS 연결을 하면 컨트롤러는 TLS 연결을 종료한다.   
    클라이언트와 컨트롤러 간의 통신은 암호화되지만 컨트롤러와 백엔드 파드 간의 통신은 암호화되지 않는다.   
    > 파드에서 실행중인 애플리케이션은 TLS 지원이 필요없다.   
    예를 들어 파드가 웹 서버를 실행하는 경우 HTTP 트래픽만 허용하고 인그레스 컨트롤러가 TLS와 관련된 모든 것을 처리하도록 할 수 있다.   
    **'시크릿'**이라는 쿠버네티스 리소스에 인증서와 개인 키를 저장하며 인그레스 매니페스트에서 참조하도록 한다.   
    다음과 같이 개인키와 인증서를 만든다.   
    ~~~
    $ openssl genrsa -out tls.key 2048
    $ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj 
      /CN=kubia.example.com
    ~~~
    두 파일로 'tls-secret'이라는 이름의 시크릿을 생성한다.   
    ~~~
    $ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
    ~~~
    인그레스 오브젝트를 업데이트하여 kubia.example.com에 대한 HTTPS요청도 수락할 수 있다.   
    인그레스 매니페스트(YAML)는 다음과 같다.   
    ~~~
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: kubia
    spec:
      tls:
      - hosts: 
        - kubia.example.com
        secretName: tls-secret
      rules:
      - host: kubia.example.com
        http:
          paths:
          - path: /
            backend:
              serviceName: kubia-nodeport
              servicePort: 80
    ~~~
    이제 HTTPS로 인그레스를 통해 서비스에 액세스할 수 있다.   
    ~~~
    $ curl -k -v https://kubia.example.com/kubia
    ~~~
    인그레스는 향후 많은 개선과 새로운 기능을 기대할 수 있다.   
    L7 로드밸런싱까지 지원하고 L4 수준의 로드밸런싱 지원도 발전가능하다.   
### 파드가 연결을 수락할 준비가 됐을 때 신호 보내기(레디니스 프로브)
파드가 만들어지자마자 즉시 요청이 들어오는 요청을 처리할 준비가 돼 있지 않았을 수 있다.   
파드가 완전히 준비될 때까지 기동 중인 파드에 요청을 전달하지 않는 것이 좋다.   
#### 레디니스 프로브 소개   
**레디니스 프로브 유형**   
**레디니스 프로브의 동작**   
#### 파드에 레디니스 프로브 추가   
