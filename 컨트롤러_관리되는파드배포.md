# 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 라이브니스 프로브: 파드를 안정적으로 유지
라이브니스 프로브를 통해 컨테이너가 살아 있는지 확인할 수 있다. 파드의 Spec에 각 컨테이너의 라이브니스 프로브를 지정할 수 있다.   
#### 3가지 메커니즘을 사용해 컨테이너에 프로브를 실행한다.    
- HTTP GET Probe: 지정한 IP, Port, 경로에 HTTP GET 요청을 수행. Probe가 응답을 수신하고 응답 코드가 오류를 나타내지 않는 경우 Probe 성공으로 간주.
- TCP Socket Probe: 컨테이너의 지정된 포트에 TCP 연결을 시도하여 연결에 성공하면 Probe 성공으로 간주.
- Exec Probe: 컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인, 상태 코드가 0이면 Probe 성공으로 간주.
#### HTTP 기반 라이브니스 프로브 생성
~~~
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia 
    livenessProbe:
      httpGet:
        path: /
        port: 8080
~~~
파드의 spec 내부에 HTTP GET livenessProbe가 포함되어있다.   
주기적으로 "/" 경로와 8080 포트에 HTTP GET을 요청해서 정상 동작인지 확인한다.   
~~~
$ kubectl get po kubia-liveness
~~~
결과로 출력되는 RESTARTS 열에는 파드의 컨테이너가 다시 시작한 횟수를 보여준다.   
~~~
$ kubectl describe po kubia-liveness
~~~
k describe 명령을 통해 다시 시작된 이유를 확인할 수 있다.   
파드 시작 시 파드의 이벤트 목록에 라이브니스 프로브 실패로 인해 컨테이너가 종료된다면, initialDelaySeconds를 적절하게 설정했는지 확인하자.   
#### 효과적인 라이브니스 프로브 생성
더 나은 라이브니스 프로브를 위해 특정 url 경로 (예를 들면 /health)에 요청하도록 Probe를 구성해 애플리케이션 내에서 실행 중인 모든 주요 구성 요소가 살아 있는지 확인하도록 구성.   
라이브니스 프로브는 애플리케이션 내부만 체크하고, 외부 요인의 영향을 받지 않도록 해야 한다. 예를 들면 웹서버에서 근본적인 원인이 데이터베이스에 있는 DB 연결 문제로 실패를 반환해서는 안된다.   
#### 프로브를 가볍게 유지하기
컨테이너가 사용할 수 있는 CPU 시간을 제한하여 프로브를 가볍게 유지할 수 있다. 추후에 자세히 다루자.   
#### 라이브니스 프로브 요약
컨테이너에 Crash가 발생하거나 라이브니스 프로브가 실패한 경우 쿠버네티스가 컨테이너를 재시작해 컨테이너가 계속 실행되도록 한다.   
이 작은 파드를 호스팅하는 노드의 Kubelet에서 수행한다. 마스터에서 실행하는 쿠버네티스 Control-Plain은 이 프로세스에 관여하지 않는다.   




## 레플리케이션컨트롤러(rc) 소개
Replication controller는 쿠버네티스 리소스로서 파드가 항상 실행되도록 보장.    
어떤 이유에서든 파드가 사라지면 rc는 사라진 파드를 감지해 교체 파드를 생성한다.   
직접 생성하여 관리되지 않는 파드는 완전히 유실될 수 있지만, rc에 의해 관리되는 파드의 경우에는 rc가 새로운 교체 파드를 생성한다.   
#### rc의 3 가지 필수 요소
- 레이블 셀렉터: rc의 범위에 있는 파드를 결정한다.
- 레플리카 수: 실행할 파드의 의도하는 개수를 지정한다.
- 파드 템플릿: 새로운 Pod Replica를 만들 때 사용된다.
#### rc 사용의 이점
- 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 한다.
- 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행 중인 모든 파드에 대한 교체 복제본이 생성된다.
- 수동 또는 자동으로 파드를 쉽게 수평으로 확장할 수 있게 한다.
#### rc 생성
다음은 ReplicationController YAML 파일 작성 예시이다.   
~~~
apiVersion: v1
kind: ReplicationController   # rc의 매니페스트 정의
metadata:
  name: kubia
spec:
  replicas: 3       # 의도하는 pod instance 개수.
  selector:         # 파드 셀렉터로 rc가 관리하는 파드를 선택할 수 있다.
    app: kubia      # 쿠버네티스는 레이블 셀렉터 app=kubia와 일치하는 pod instance가 3개를 유지하도록 하는 kubia라는 이름의 새로운 ReplicationController을 생성한다.
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
~~~
k create 명령으로 생성한 후 rc의 작동을 확인하자.   
~~~
$ kubectl get pods
~~~
위 명령어로 파드의 수를 확인해보면 3개가 만들어지는 것을 확인할 수 있다.   
rc는 이제 생성된 3개 파드를 관리한다.   
~~~
$ kubectl delete pod <pod_name>
~~~
위 명령어로 rc가 관리하는 pod 하나를 수동으로 삭제해서 rc의 반응을 살펴보자.   
바로 pod를 다시 조회하면 pod 4개가 조회된다. 이는 삭제하는 파드가 종료 중(Terminating)이고 새 pod는 이미 생성됐기 때문이다.   
#### rc 정보 얻기
~~~
$ kubectl get rc
$ kubectl describe rc kubia
~~~
#### 컨트롤러가 새로운 파드를 생성한 원인 이해하기
컨트롤러는 부족한 파드 수에 대해 대응한다.
rc는 삭제되는 파드에 대해 즉시 통지를 받지만, 이 통지 자체가 대체 파드를 생성하게 하는 것은 아니다.   
이 통지는 컨트롤러가 실제 파드 수를 확인하고, 적절한 조치를 취하도록 하는 __Trigger 역할이다.__   
#### 노드 장애 대응
rc는 노드의 파드가 다운됐음을 감지하자마자 파드를 대체하기 위해 새 파드를 가동한다.   
쿠버네티스는 노드에 장애가 발생하면 해당 노드에서 실행 중이던 애플리케이션을 자동으로 다른 시스템에 마이그레이션한다.
#### rc의 범위 안팎으로 파드 이동하기
파드의 레이블을 변경하여 파드를 rc의 범위에서 제거하거나 추가힐 수 있다. 한 rc에서 다른 rc로 이동할 수도 있다.   
다음 명령어로 각 파드의 레이블을 확인하고 레이블을 수정할 수 있다.
~~~
$ kubectl get pods --show-labels
$ kubectl label pod <pod_name> <label=value> --overwrite
~~~
--overwrite 옵션을 사용해야 기존의 레이블이 변경된다.   
파드의 레이블을 rc에서 관리하지 않는 레이블로 변경하면, rc의 범위에서 제거된다.   
rc의 범위에서 제거하여 만약 rc에서 목표하는 replica 개수가 아니라면 rc는 새로운 대체 파드를 생성한다.   
#### 파드 템플릿 변경
밥먹으러감


## 레플리카셋

## 데몬셋

## 잡 리소스

## 크론잡
