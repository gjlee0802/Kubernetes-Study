# 레플리케이션과 그 밖의 컨트롤러: 관리되는 파드 배포

## 라이브니스 프로브: 파드를 안정적으로 유지
라이브니스 프로브를 통해 컨테이너가 살아 있는지 확인할 수 있다. 파드의 Spec에 각 컨테이너의 라이브니스 프로브를 지정할 수 있다.   
#### 3가지 메커니즘을 사용해 컨테이너에 프로브를 실행한다.    
- HTTP GET Probe: 지정한 IP, Port, 경로에 HTTP GET 요청을 수행. Probe가 응답을 수신하고 응답 코드가 오류를 나타내지 않는 경우 Probe 성공으로 간주.
- TCP Socket Probe: 컨테이너의 지정된 포트에 TCP 연결을 시도하여 연결에 성공하면 Probe 성공으로 간주.
- Exec Probe: 컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인, 상태 코드가 0이면 Probe 성공으로 간주.
#### HTTP 기반 라이브니스 프로브 생성
~~~
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia 
    livenessProbe:
      httpGet:
        path: /
        port: 8080
~~~
파드의 spec 내부에 HTTP GET livenessProbe가 포함되어있다.   
주기적으로 "/" 경로와 8080 포트에 HTTP GET을 요청해서 정상 동작인지 확인한다.   
~~~
$ kubectl get po kubia-liveness
~~~
결과로 출력되는 RESTARTS 열에는 파드의 컨테이너가 다시 시작한 횟수를 보여준다.   
~~~
$ kubectl describe po kubia-liveness
~~~
k describe 명령을 통해 다시 시작된 이유를 확인할 수 있다.   
파드 시작 시 파드의 이벤트 목록에 라이브니스 프로브 실패로 인해 컨테이너가 종료된다면, initialDelaySeconds를 적절하게 설정했는지 확인하자.   
#### 효과적인 라이브니스 프로브 생성
더 나은 라이브니스 프로브를 위해 특정 url 경로 (예를 들면 /health)에 요청하도록 Probe를 구성해 애플리케이션 내에서 실행 중인 모든 주요 구성 요소가 살아 있는지 확인하도록 구성.   
라이브니스 프로브는 애플리케이션 내부만 체크하고, 외부 요인의 영향을 받지 않도록 해야 한다. 예를 들면 웹서버에서 근본적인 원인이 데이터베이스에 있는 DB 연결 문제로 실패를 반환해서는 안된다.   
#### 프로브를 가볍게 유지하기
컨테이너가 사용할 수 있는 CPU 시간을 제한하여 프로브를 가볍게 유지할 수 있다. 추후에 자세히 다루자.   
#### 라이브니스 프로브 요약
컨테이너에 Crash가 발생하거나 라이브니스 프로브가 실패한 경우 쿠버네티스가 컨테이너를 재시작해 컨테이너가 계속 실행되도록 한다.   
이 작은 파드를 호스팅하는 노드의 Kubelet에서 수행한다. 마스터에서 실행하는 쿠버네티스 Control-Plain은 이 프로세스에 관여하지 않는다.   




## 레플리케이션컨트롤러(rc) 소개, 구시대적 컨트롤러(이젠 RS를 사용!)
Replication controller는 쿠버네티스 리소스로서 파드가 항상 실행되도록 보장.    
어떤 이유에서든 파드가 사라지면 rc는 사라진 파드를 감지해 교체 파드를 생성한다.   
직접 생성하여 관리되지 않는 파드는 완전히 유실될 수 있지만, rc에 의해 관리되는 파드의 경우에는 rc가 새로운 교체 파드를 생성한다.   
#### rc의 3 가지 필수 요소
- 레이블 셀렉터: rc의 범위에 있는 파드를 결정한다.
- 레플리카 수: 실행할 파드의 의도하는 개수를 지정한다.
- 파드 템플릿: 새로운 Pod Replica를 만들 때 사용된다.
#### rc 사용의 이점
- 기존 파드가 사라지면 새 파드를 시작해 파드가 항상 실행되도록 한다.
- 클러스터 노드에 장애가 발생하면 장애가 발생한 노드에서 실행 중인 모든 파드에 대한 교체 복제본이 생성된다.
- 수동 또는 자동으로 파드를 쉽게 수평으로 확장할 수 있게 한다.
#### rc 생성
다음은 ReplicationController YAML 파일 작성 예시이다.   
~~~
apiVersion: v1
kind: ReplicationController   # rc의 매니페스트 정의
metadata:
  name: kubia
spec:
  replicas: 3       # 의도하는 pod instance 개수.
  selector:         # 파드 셀렉터로 rc가 관리하는 파드를 선택할 수 있다.
    app: kubia      # 쿠버네티스는 레이블 셀렉터 app=kubia와 일치하는 pod instance가 3개를 유지하도록 하는 kubia라는 이름의 새로운 ReplicationController을 생성한다.
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
~~~
k create 명령으로 생성한 후 rc의 작동을 확인하자.   
~~~
$ kubectl get pods
~~~
위 명령어로 파드의 수를 확인해보면 3개가 만들어지는 것을 확인할 수 있다.   
rc는 이제 생성된 3개 파드를 관리한다.   
~~~
$ kubectl delete pod <pod_name>
~~~
위 명령어로 rc가 관리하는 pod 하나를 수동으로 삭제해서 rc의 반응을 살펴보자.   
바로 pod를 다시 조회하면 pod 4개가 조회된다. 이는 삭제하는 파드가 종료 중(Terminating)이고 새 pod는 이미 생성됐기 때문이다.   
#### rc 정보 얻기
~~~
$ kubectl get rc
$ kubectl describe rc kubia
~~~
#### 컨트롤러가 새로운 파드를 생성한 원인 이해하기
컨트롤러는 부족한 파드 수에 대해 대응한다.
rc는 삭제되는 파드에 대해 즉시 통지를 받지만, 이 통지 자체가 대체 파드를 생성하게 하는 것은 아니다.   
이 통지는 컨트롤러가 실제 파드 수를 확인하고, 적절한 조치를 취하도록 하는 __Trigger 역할이다.__   
#### 노드 장애 대응
rc는 노드의 파드가 다운됐음을 감지하자마자 파드를 대체하기 위해 새 파드를 가동한다.   
쿠버네티스는 노드에 장애가 발생하면 해당 노드에서 실행 중이던 애플리케이션을 자동으로 다른 시스템에 마이그레이션한다.
#### rc의 범위 안팎으로 파드 이동하기
파드의 레이블을 변경하여 파드를 rc의 범위에서 제거하거나 추가힐 수 있다. 한 rc에서 다른 rc로 이동할 수도 있다.   
다음 명령어로 각 파드의 레이블을 확인하고 레이블을 수정할 수 있다.
~~~
$ kubectl get pods --show-labels
$ kubectl label pod <pod_name> <label=value> --overwrite
~~~
--overwrite 옵션을 사용해야 기존의 레이블이 변경된다.   
파드의 레이블을 rc에서 관리하지 않는 레이블로 변경하면, rc의 범위에서 제거된다.   
rc의 범위에서 제거하여 만약 rc에서 목표하는 replica 개수가 아니라면 rc는 새로운 대체 파드를 생성한다.   
#### 파드 템플릿 변경
rc의 파드 템플릿은 언제든지 수정할 수 있다.   
파드 템플릿를 변경하는 것은 쿠키 커터를 다른 것으로 교체하는 것과 같다.    
기존 파드를 수정하려면 해당 파드를 삭제하고 rc가 새 템플릿을 기반으로 새 파드로 교체하도록 해야 한다.   
~~~
$ kubectl edit rc kubia
~~~
파드 템플릿 섹션을 찾아 메타데이터에 레이블을 추가한다. 저장하고 편집기를 종료하면 rc가 업데이트된다.
기존의 파드를 삭제하면 rc는 변경된 파드 템플릿으로 새로운 파드를 생성할 것이다.   
기존 파드를 새로운 파드로 업그레이드하는 방법은 추후에 배우게 될 것이다.   
#### 수평 파드 스케일링
rc는 특정 수의 파드 인스턴스를 항상 실행하도록 보장한다.   
복제본 수를 변경하는 것은 매우 쉽다. 이는 파드를 수평으로 확장하는 것이 무척 쉽다는 것을 의미한다.   
rc 리소스의 replicas 필드 값을 변경하기만 하면 된다.   
~~~
$ kubectl scale rc kubia --replicas=10
~~~
scale rc 명령으로 replicas 필드를 변경하거나 다음과 같이 rc의 정의를 편집해서 replicas를 수정할 수 있다.   
~~~
$ kubectl edit rc kubia
~~~
#### rc 삭제
- rc와 함께 관리하는 파드들도 삭제하는 경우   
  ~~~
  $ kubectl delete rc kubia
  ~~~
- rc만 삭제하고 관리하던 파드들은 그대로 두고 싶은 경우 (파드들은 어디에도 속하지 않게 된다)   
  ~~~
  $ kubectl delete rc kubia --cascade=false
  ~~~
  
  
  
  
## 레플리카셋(rs)
ReplicaSet은 차세대 rc이며 rc를 완전히 대체할 것이다.   
일반적으로 ReplicaSet을 직접 생성하지는 않고, 추후에 배울 상위 수준의 디플로이먼트 리소스를 생성할 때 자동으로 생성되게 한다.
#### ReplicaSet과 ReplicationController 비교
레플리카셋은 rc와 똑같이 동작하지만 좀 더 풍부한 표현식을 사용하는 파드 셀렉터를 갖고 있다.   
rc의 레이블 셀렉터는 특정 레이블이 있는 파드만을 매칭시킬 수 있는 반면,   
ReplicaSet의 셀렉터는 특정 레이블이 없는 파드나 레이블의 값과 상관없이 특정 레이블의 키를 갖는 파드를 매칭시킬 수 있다.   
#### ReplicaSet 정의하기
다음과 같이 ReplicaSet을 위한 YAML파일을 만들어 작성한다.   
~~~
apiVersion: apps/v1beta2    # ReplicaSet이 v1 API의 일부가 아님에 주의 (API 그룹/API 버전)
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:            # rc와 달리 ReplicaSet은 selector.matchLabels 아래에 지정한다.
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
~~~
#### ReplicaSet의 더욱 표현적인 레이블 셀렉터 사용하기
앞에서 처음 정의한 rs의 YAML파일에서는 단순한 matchLabels 셀렉터를 사용했다.   
matchExpression을 사용하도록 셀렉터를 다시 작성하자.   
~~~
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
         - kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
~~~
matchExpressions 셀렉터의 표현식은 key, operator, values로 구성된다.   
다음은 4가지의 유효한 연산자다.   
- In: 레이블의 값이 지정된 값 중 하나와 일치해야 한다.   
- NotIn: 레이블의 값이 지정된 값과 일치하지 않아야 한다.   
- Exists: 파드는 지정된 키를 가진 레이블이 포함돼야 한다. 이 연산자를 사용할 때는 값(value) 필드를 지정하지 않아야 한다.   
- DoesNotExist: 파드에 지정된 키를 가진 레이블이 포함돼 있지 않아야 한다. 값 필드를 지정하지 않아야 한다.   




## 데몬셋
클러스터의 모든 노드에, 노드당 하나의 파드만 실행되길 원하는 경우가 있을 수 있다.   
데몬셋은 각 노드에서 하나의 파드 복제본만 실행하도록 한다.   
시스템 수준의 작업을 수행하는 인프라 관련 파드가 이런 경우다.   
예를 들면 모든 노드에서 로그 수집가와 리소스 모니터를 실행하려는 경우가 좋은 예다.   
#### 데몬셋으로 모든 노드에 파드 실행하기
모든 클러스터 노드마다 파드를 하나만 실행하려면 데몬셋 오브젝트를 생성한다.   
데몬셋에 의해 생성되는 파드는 타깃 노드가 이미 지정돼 있고 쿠버네티스 스케줄러를 건너뛰는 것을 제외하면 이 오브젝트는 rc 또는 rs와 매우 유사하다.   
파드가 클러스터 내에 무작위로 흩어져 배포되지 않는다.   
데몬셋에는 원하는 복제본 수(replicas) 개념이 없다.   
노드가 다운되면 데몬셋은 다른 곳에서 파드를 생성하지 않는다. 그러나 새로운 노드가 클러스터에 추가되면 데몬셋은 즉시 새 파드 인스턴스를 새 노드에 배포한다.   
또한 파드가 삭제되면 데몬셋은 ReplicaSet과 마찬가지로 그 안에 구성된 파드 템플릿으로 파드를 생성한다.   
#### 데몬셋을 사용해 특정 노드에서만 파드를 실행하기
파드가 노드의 일부에서만 실행되도록 지정하지 않으면 데몬셋은 클러스터의 모든 노드에 파드를 배포한다.   
데몬셋 정의의 일부인 파드 템플릿에서 node-Selector 속성을 지정하면 된다.   
#### 데몬셋 YAML 정의로 생성
~~~
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:   # 파드 템플릿은 disk=ssd 레이블이 있는 노드를 선택하는 노드 셀렉터를 갖는다.
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
~~~
다음 명령어로 데몬셋을 생성하고 확인한다.
~~~
$ kubectl create -f <YAML_file>
$ kubectl get ds
~~~
노드에 해당되는 레이블(disk=ssd)를 추가하지 않았다면 파드가 배포되지 않는다.   
#### 필요한 레이블 노드에 추가하기
다음 kubectl label 명령어를 입력하여 라벨을 추가한다.   
~~~
$ kubectl label node <node_name> <key=value>
~~~




## 잡 리소스
rc, rs, ds는 계속 실행되는 지속적인 태스크를 실행한다.   
그러나 완료 가능한 태스크에서는 프로세스가 종료된 후에 다시 시작되지 않는다.    
Job 리소스는 이러한 기능을 지원한다. 파드의 컨테이너 내에서 프로세스가 성공적으로 완료되면 컨테이너를 다시 시작하지 않는 파드를 실행할 수 있다.   
일단 프로세스가 성공적으로 완료되면 파드는 완료된 것으로 간주된다.   
프로세스 자체에 장애가 발생한 경우, 잡에서 컨테이너를 다시 시작할 것인지 설정할 수 있다.   
잡의 예로는 데이터를 어딘가에 저장하고 있고, 이 데이터를 변환해서 어딘가로 전송해야하는 경우를 들 수 있다.   
#### Job 리소스 정의
다음과 같이 Job Manifest를 만든다.   
~~~
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
~~~
restartPolicy의 기본값은 Always이다. restartPolicy 필드는 컨테이너에서 실행 중인 프로세스가 종료될 때 쿠버네티스가 수행할 작업을 지정할 수 있다.   
Always/OnFailure/Never을 설정할 수 있다.   
### 파드를 실행한 잡 보기
~~~
$ kubectl get jobs
$ kubectl get po -a
~~~
파드 목록을 조회할 때 --show-all (-a) 옵션을 사용하지 않으면 완료된 파드는 표시되지 않는다.   
파드가 완료될 때 파드가 삭제되지 않은 이유는 해당 파드의 로그를 검사할 수 있게 하기 위해서다.   
~~~
$ kubectl logs <pod_name>
~~~
#### Job에서 여러 파드 인스턴스 실행하기
Job은 두 개 이상의 파드 인스턴스를 생성해 병렬 또는 순차적으로 실행하도록 구성할 수 있다.   
Job의 spec에 completions와 parallelism 속성을 설정해 수행한다.   
- 순차적으로 Job 파드 실행하기   
Job의 파드를 몇 번 실행할지를 completions에 설정한다.   
  ~~~
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: multi-completion-batch-job
  spec:
    completions: 5
    template:
      metadata:
        labels:
          app: batch-job
      spec:
        restartPolicy: OnFailure
        containers:
        - name: main
          image: luksa/batch-job
  ~~~
이 Job은 차례로 다섯 개의 파드를 실행한다.   
처음에는 파드를 하나 만들고, 파드의 컨테이너가 완료되면 두 번째 파드를 만들어 성공적으로 5번 완료될 때까지 과정을 반복한다.   
파드 중 하나가 실패하면 Job이 새 파드를 생성한다.   
- 병렬로 Job 파드 실행하기   
Job 파드를 하나씩 차례로 실행하는 대신 Job이 여러 파드를 병렬로 실행할 수 있다.   
parallelism 속성을 이용해 병렬로 실행할 파드 수를 지정한다.   
  ~~~
  apiVersion: batch/v1
  kind: Job
  metadata:
    name: multi-completion-batch-job
  spec:
    completions: 5
    parallelism: 2
    template:
      metadata:
        labels:
          app: batch-job
      spec:
        restartPolicy: OnFailure
        containers:
        - name: main
          image: luksa/batch-job
  ~~~
parallism을 2로 설정하여 파드를 2개 잡고 병렬로 실행하는 것이다.   
이 2개의 파드 중 하나가 완료하면 다섯 개의 파드가 성공적으로 완료될 때까지 Job이 다음 파드를 실행한다. (completions: 5)   
#### Job 스케일링
Job이 실행되는 동안 Job의 parallelism 속성을 변경할 수도 있다.   
RS나 RC를 스케일링 하는 것과 유사하다.   
다음과 같이 kubectl scale 명령을 사용해 수행할 수 있다.   
~~~
$ kubectl scale job multi-completion-batch-job --replicas 3
~~~
parallelism을 2에서 3으로 증가시켰기 때문에 다른 파드가 즉시 가동되어 세 개의 파드가 실행되게 된다.
#### Job 파드가 완료되는 데 걸리는 시간 제한하기
파드 spec에 activeDeadlineSeconds 속성을 설정해 파드의 실행 시간을 제할할 수 있다.   
이보다 오래 실행되면 시스템이 종료를 시도하고 Job을 실패한 것으로 표시한다.   


## 크론잡(Cron-Job)
#### Job을 주기적으로 또는 한 번 실행되도록 스케줄링하기
Job 리소스를 생성하면 즉시 해당하는 파드를 실행한다.   
그러나 많은 Batch-Job이 미래의 특정 시간 혹은 지정된 간격으로 반복 실행해야 한다.   
이러한 작업들을 **크론(Cron)** 작업이라 한다.   
쿠버네티스에서의 크론 작업은 크론잡 리소스를 만들어 구성한다.   
~~~
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"    # 매일, 매시간 0,15,30,45분에 실행해야 한다.
  jobTemplate:                      # 아래는 크론잡이 생성하는 잡 리소스의 템플릿.
    spec:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job
~~~
#### 스케줄 설정하기
스케줄은 왼쪽에서 오른쪽으로 다섯 개의 항목을 갖고 있다.
- 분
- 시
- 일
- 월
- 요일
이 예제에서는 15분마다 잡을 실행하고자 하므로, 스케줄은 "0,15,30,45 \* \* \* \*"이어야 한다.   
이는 매시간(1st \*), 매일(2nd \*), 매월(3rd \*) 모든 요일(4th \*)의 0, 15, 30, 45분에 실행됨을 의미한다.   
매달 첫째 날에 30분마다 실행하고 싶다면, 스케줄을 "0,30 \* 1 \* \*"로 설정해야한다.   
일요일 3AM마다 실행하고 싶다면, "0 3 \* \* 0"로 설정해야 한다.   
#### 스케줄된 Job의 실행 방법 이해
Job 리소스는 대략 예정된 시간에 크론잡 리소스에서 생성된다. 그리고 Job은 파드를 생성한다.   
Job이나 파드가 상대적으로 늦게 생성되고 실행될 수 있다. 예정된 시간을 너무 초과하여 시작하는 것을 막기 위해서는 다음 예제처럼 CronJob spec의 startingDeadlineSeconds 필드를 지정해 데드라인을 설정할 수 있다.   
~~~
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"    # 매일, 매시간 0,15,30,45분에 실행해야 한다.
  startingDeadlineSeconds: 15
~~~
